when do we say a function is continuous,"In mathematics, a continuous function is a function for which sufficiently small changes in the input result in arbitrarily small changes in the output. Otherwise, a function is said to be a discontinuous function. A continuous function with a continuous inverse function is called a homeomorphism.",,,,,,,,,,
when did the information age start and end,"The Information Age (also known as the Computer Age, Digital Age, or New Media Age) is a period in human history characterized by the shift from traditional industry that the Industrial Revolution brought through industrialization, to an economy based on information computerization. The onset of the Information Age is associated with the Digital Revolution, just as the Industrial Revolution marked the onset of the Industrial Age. The definition of what digital means (or what information means) continues to change over time as new technologies, user devices, methods of interaction with other humans and devices enter the domain of research, development and market launch.","The ONLY authorized website by Dr. Delbert Blair! Dismiss No products in the cart. No products in the cart. It looks like nothing was found at this location. Maybe try one of the links below or a search? Your personal data will be used to support your experience throughout this website, to manage access to your account, and for other purposes described in our privacy policy.","The Information Age (also known as the Computer Age, Digital Age, Silicon Age, or New Media Age) is a historical period that began in the mid-20th century. It is characterized by a rapid shift from traditional industries, as established during the Industrial Revolution, to an economy centered on information technology.[1] The onset of the Information Age has been linked to the development of the transistor in 1947,[1] the optical amplifier in 1957,[2] and Unix time,[3] which began on January 1, 1970. These technological advances have had a significant impact on the way information is processed and transmitted. According to the United Nations Public Administration Network, the Information Age was formed by capitalizing on computer microminiaturization advances,[4] which led to modernized information systems and internet communications as the driving force of social evolution.[5] Library expansion was calculated in 1945 by Fremont Rider to double in capacity every 16 years where sufficient space made available.[6] He advocated replacing bulky, decaying printed works with miniaturized microform analog photographs, which could be duplicated on-demand for library patrons and other institutions. Rider did not foresee, however, the digital technology that would follow decades later to replace analog microform with digital imaging, storage, and transmission media, whereby vast increases in the rapidity of information growth would be made possible through automated, potentially-lossless digital technologies. Accordingly, Moore's law, formulated around 1965, would calculate that the number of transistors in a dense integrated circuit doubles approximately every two years.[7][8] By the early 1980s, along with improvements in computing power, the proliferation of the smaller and less expensive personal computers allowed for immediate access to information and the ability to share and store it. Connectivity between computers within organizations enabled access to greater amounts of information. The world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes (EB) in 1986 to 15.8 EB in 1993; over 54.5 EB in 2000; and to 295 (optimally compressed) EB in 2007.[9][10] This is the informational equivalent to less than one 730-megabyte (MB) CD-ROM per person in 1986 (539 MB per person); roughly four CD-ROM per person in 1993; twelve CD-ROM per person in the year 2000; and almost sixty-one CD-ROM per person in 2007.[11] It is estimated that the world's capacity to store information has reached 5 zettabytes in 2014,[12] the informational equivalent of 4,500 stacks of printed books from the earth to the sun. The amount of digital data stored appears to be growing approximately exponentially, reminiscent of Moore's law. As such, Kryder's law prescribes that the amount of storage space available appears to be growing approximately exponentially.[13][14][15][8] The world's technological capacity to receive information through one-way broadcast networks was 432 exabytes of (optimally compressed) information in 1986; 715 (optimally compressed) exabytes in 1993; 1.2 (optimally compressed) zettabytes in 2000; and 1.9 zettabytes in 2007, the information equivalent of 174 newspapers per person per day.[11] The world's effective capacity to exchange information through two-way telecommunication networks was 281 petabytes of (optimally compressed) information in 1986; 471 petabytes in 1993; 2.2 (optimally compressed) exabytes in 2000; and 65 (optimally compressed) exabytes in 2007, the information equivalent of six newspapers per person per day.[11] In the 1990s, the spread of the Internet caused a sudden leap in access to and ability to share information in businesses and homes globally. A computer that cost $3000 in 1997 would cost $2000 two years later and $1000 the following year, due to the rapid advancement of technology. The world's technological capacity to compute information with humanly guided general-purpose computers grew from 3.0 × 108 MIPS in 1986, to 4.4 × 109 MIPS in 1993; to 2.9 × 1011 MIPS in 2000; to 6.4 × 1012 MIPS in 2007.[11] An article featured in the journal Trends in Ecology and Evolution in 2016 reported that:[12] Digital technology has vastly exceeded the cognitive capacity of any single human being and has done so a decade earlier than predicted. In terms of capacity, there are two measures of importance: the number of operations a system can perform and the amount of information that can be stored. The number of synaptic operations per second in a human brain has been estimated to lie between 10^15 and 10^17. While this number is impressive, even in 2007 humanity's general-purpose computers were capable of performing well over 10^18 instructions per second. Estimates suggest that the storage capacity of an individual human brain is about 10^12 bytes. On a per capita basis, this is matched by current digital storage (5x10^21 bytes per 7.2x10^9 people). Genetic code may also be considered part of the information revolution. Now that sequencing has been computerized, genome can be rendered and manipulated as data. This started with DNA sequencing, invented by Walter Gilbert and Allan Maxam[16] in 1976-1977 and Frederick Sanger in 1977, grew steadily with the Human Genome Project, initially conceived by Gilbert and finally, the practical applications of sequencing, such as gene testing, after the discovery by Myriad Genetics of the BRCA1 breast cancer gene mutation. Sequence data in Genbank has grown from the 606 genome sequences registered in December 1982 to the 231 million genomes in August 2021. An additional 13 trillion incomplete sequences are registered in the Whole Genome Shotgun submission database as of August 2021. The information contained in these registered sequences has doubled every 18 months.[17] During rare times in human history, there have been periods of innovation that have transformed human life. The Neolithic Age, the Scientific Age and the Industrial Age all, ultimately, induced discontinuous and irreversible changes in the economic, social and cultural elements of the daily life of most people. Traditionally, these epochs have taken place over hundreds, or in the case of the Neolithic Revolution, thousands of years, whereas the Information Age swept to all parts of the globe in just a few years. The reason for its rapid adoption is the rapidly advancing speed of information exchange. Between 7,000 and 10,000 years ago during the Neolithic period, humans began to domesticate animals, began to farm grains and to replace stone tools with ones made of metal. These innovations allowed nomadic hunter-gatherers to settle down. Villages formed along the Yangtze River in China in 6,500 B.C., the Nile River region of Africa and in Mesopotamia (Iraq) in 6,000 B.C. Cities emerged between 6,000 B.C. and 3,500 B.C. The development of written communication (cuneiform in Sumeria and hieroglyphs in Egypt in 3,500 B.C. and writing in Egypt in 2,560 B.C. and in Minoa and China around 1,450 B.C.) enabled ideas to be preserved for extended periods to spread extensively. In all, Neolithic developments, augmented by writing as an information tool, laid the groundwork for the advent of civilization. The Scientific Age began in the period between Galileo's 1543 proof that the planets orbit the sun and Newton's publication of the laws of motion and gravity in Principia in 1697. This age of discovery continued through the 18th century, accelerated by widespread use of the moveable type printing press by Johannes Gutenberg. The Industrial Age began in Great Britain in 1760 and continued into the mid-19th century. It altered many aspects of life around the world. The invention of machines such as the mechanical textile weaver by Edmund Cartwrite, the rotating shaft steam engine by James Watt and the cotton gin by Eli Whitney, along with processes for mass manufacturing, came to serve the needs of a growing global population. The Industrial Age harnessed steam and waterpower to reduce the dependence on animal and human physical labor as the primary means of production. Thus, the core of the Industrial Revolution was the generation and distribution of energy from coal and water to produce steam and, later in the 20th century, electricity. The Information Age also requires electricity to power the global networks of computers that process and store data. However, what dramatically accelerated the pace of adoption of The Information Age, as compared to previous ones, was the speed by which knowledge could be transferred and pervaded the entire human family in a few short decades. This acceleration came about with the adoptions of a new form of power. Beginning in 1972, engineers devised ways to harness light to convey data through fiber optic cable. Today, light-based optical networking systems at the heart of telecom networks and the Internet span the globe and carry most of the information traffic to and from users and data storage systems. There are different conceptualizations of the Information Age. Some focus on the evolution of information over the ages, distinguishing between the Primary Information Age and the Secondary Information Age. Information in the Primary Information Age was handled by newspapers, radio and television. The Secondary Information Age was developed by the Internet, satellite televisions and mobile phones. The Tertiary Information Age was emerged by media of the Primary Information Age interconnected with media of the Secondary Information Age as presently experienced.[18][19][20] Others classify it in terms of the well-established Schumpeterian long waves or Kondratiev waves. Here authors distinguish three different long-term metaparadigms, each with different long waves. The first focused on the transformation of material, including stone, bronze, and iron. The second, often referred to as industrial revolution, was dedicated to the transformation of energy, including water, steam, electric, and combustion power. Finally, the most recent metaparadigm aims at transforming information. It started out with the proliferation of communication and stored data and has now entered the age of algorithms, which aims at creating automated processes to convert the existing information into actionable knowledge.[21] Eventually, Information and communication technology (ICT)—i.e. computers, computerized machinery, fiber optics, communication satellites, the Internet, and other ICT tools—became a significant part of the world economy, as the development of optical networking and microcomputers greatly changed many businesses and industries.[22][23] Nicholas Negroponte captured the essence of these changes in his 1995 book, Being Digital, in which he discusses the similarities and differences between products made of atoms and products made of bits.[24] The Information Age has affected the workforce in several ways, such as compelling workers to compete in a global job market. One of the most evident concerns is the replacement of human labor by computers that can do their jobs faster and more effectively, thus creating a situation in which individuals who perform tasks that can easily be automated are forced to find employment where their labor is not as disposable.[25] This especially creates issue for those in industrial cities, where solutions typically involve lowering working time, which is often highly resisted. Thus, individuals who lose their jobs may be pressed to move up into joining ""mind workers"" (e.g. engineers, doctors, lawyers, teachers, professors, scientists, executives, journalists, consultants), who are able to compete successfully in the world market and receive (relatively) high wages.[citation needed] Along with automation, jobs traditionally associated with the middle class (e.g. assembly line, data processing, management, and supervision) have also begun to disappear as result of outsourcing.[26] Unable to compete with those in developing countries, production and service workers in post-industrial (i.e. developed) societies either lose their jobs through outsourcing, accept wage cuts, or settle for low-skill, low-wage service jobs.[26] In the past, the economic fate of individuals would be tied to that of their nation's. For example, workers in the United States were once well paid in comparison to those in other countries. With the advent of the Information Age and improvements in communication, this is no longer the case, as workers must now compete in a global job market, whereby wages are less dependent on the success or failure of individual economies.[26] In effectuating a globalized workforce, the internet has just as well allowed for increased opportunity in developing countries, making it possible for workers in such places to provide in-person services, therefore competing directly with their counterparts in other nations. This competitive advantage translates into increased opportunities and higher wages.[27] The Information Age has affected the workforce in that automation and computerization have resulted in higher productivity coupled with net job loss in manufacturing. In the United States, for example, from January 1972 to August 2010, the number of people employed in manufacturing jobs fell from 17,500,000 to 11,500,000 while manufacturing value rose 270%.[28] Although it initially appeared that job loss in the industrial sector might be partially offset by the rapid growth of jobs in information technology, the recession of March 2001 foreshadowed a sharp drop in the number of jobs in the sector. This pattern of decrease in jobs would continue until 2003,[29] and data has shown that, overall, technology creates more jobs than it destroys even in the short run.[30] Industry has become more information-intensive while less labor- and capital-intensive. This has left important implications for the workforce, as workers have become increasingly productive as the value of their labor decreases. For the system of capitalism itself, the value of labor decreases, the value of capital increases. In the classical model, investments in human and financial capital are important predictors of the performance of a new venture.[31] However, as demonstrated by Mark Zuckerberg and Facebook, it now seems possible for a group of relatively inexperienced people with limited capital to succeed on a large scale.[32] The Information Age was enabled by technology developed in the Digital Revolution, which was itself enabled by building on the developments of the Technological Revolution. The onset of the Information Age can be associated with the development of transistor technology.[1] The concept of a field-effect transistor was first theorized by Julius Edgar Lilienfeld in 1925.[33] The first practical transistor was the point-contact transistor, invented by the engineers Walter Houser Brattain and John Bardeen while working for William Shockley at Bell Labs in 1947. This was a breakthrough that laid the foundations for modern technology.[1] Shockley's research team also invented the bipolar junction transistor in 1952.[34][33] The most widely used type of transistor is the metal–oxide–semiconductor field-effect transistor (MOSFET), invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1960.[35] The complementary MOS (CMOS) fabrication process was developed by Frank Wanlass and Chih-Tang Sah in 1963.[36] Before the advent of electronics, mechanical computers, like the Analytical Engine in 1837, were designed to provide routine mathematical calculation and simple decision-making capabilities. Military needs during World War II drove development of the first electronic computers, based on vacuum tubes, including the Z3, the Atanasoff–Berry Computer, Colossus computer, and ENIAC. The invention of the transistor enabled the era of mainframe computers (1950s–1970s), typified by the IBM 360. These large, room-sized computers provided data calculation and manipulation that was much faster than humanly possible, but were expensive to buy and maintain, so were initially limited to a few scientific institutions, large corporations, and government agencies. The germanium integrated circuit (IC) was invented by Jack Kilby at Texas Instruments in 1958.[37] The silicon integrated circuit was then invented in 1959 by Robert Noyce at Fairchild Semiconductor, using the planar process developed by Jean Hoerni, who was in turn building on Mohamed Atalla's silicon surface passivation method developed at Bell Labs in 1957.[38][39] Following the invention of the MOS transistor by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959,[35] the MOS integrated circuit was developed by Fred Heiman and Steven Hofstein at RCA in 1962.[40] The silicon-gate MOS IC was later developed by Federico Faggin at Fairchild Semiconductor in 1968.[41] With the advent of the MOS transistor and the MOS IC, transistor technology rapidly improved, and the ratio of computing power to size increased dramatically, giving direct access to computers to ever smaller groups of people. The first commercial single-chip microprocessor launched in 1971, the Intel 4004, which was developed by Federico Faggin using his silicon-gate MOS IC technology, along with Marcian Hoff, Masatoshi Shima and Stan Mazor.[42][43] Along with electronic arcade machines and home video game consoles pioneered by Nolan Bushnell in the 1970s, the development of personal computers like the Commodore PET and Apple II (both in 1977) gave individuals access to the computer. But data sharing between individual computers was either non-existent or largely manual, at first using punched cards and magnetic tape, and later floppy disks. The first developments for storing data were initially based on photographs, starting with microphotography in 1851 and then microform in the 1920s, with the ability to store documents on film, making them much more compact. Early information theory and Hamming codes were developed about 1950, but awaited technical innovations in data transmission and storage to be put to full use. Magnetic-core memory was developed from the research of Frederick W. Viehe in 1947 and An Wang at Harvard University in 1949.[44][45] With the advent of the MOS transistor, MOS semiconductor memory was developed by John Schmidt at Fairchild Semiconductor in 1964.[46][47] In 1967, Dawon Kahng and Simon Sze at Bell Labs described in 1967 how the floating gate of an MOS semiconductor device could be used for the cell of a reprogrammable ROM.[48] Following the invention of flash memory by Fujio Masuoka at Toshiba in 1980,[49][50] Toshiba commercialized NAND flash memory in 1987.[51][48] Copper wire cables transmitting digital data connected computer terminals and peripherals to mainframes, and special message-sharing systems leading to email, were first developed in the 1960s. Independent computer-to-computer networking began with ARPANET in 1969. This expanded to become the Internet (coined in 1974). Access to the Internet improved with the invention of the World Wide Web in 1991. The capacity expansion from dense wave division multiplexing, optical amplification and optical networking in the mid-1990s led to record data transfer rates. By 2018, optical networks routinely delivered 30.4 terabits/s over a fiber optic pair, the data equivalent of 1.2 million simultaneous 4K HD video streams.[52] MOSFET scaling, the rapid miniaturization of MOSFETs at a rate predicted by Moore's law,[53] led to computers becoming smaller and more powerful, to the point where they could be carried. During the 1980s–1990s, laptops were developed as a form of portable computer, and personal digital assistants (PDAs) could be used while standing or walking. Pagers, widely used by the 1980s, were largely replaced by mobile phones beginning in the late 1990s, providing mobile networking features to some computers. Now commonplace, this technology is extended to digital cameras and other wearable devices. Starting in the late 1990s, tablets and then smartphones combined and extended these abilities of computing, mobility, and information sharing. Metal–oxide–semiconductor (MOS) image sensors, which first began appearing in the late 1960s, led to the transition from analog to digital imaging, and from analog to digital cameras, during the 1980s–1990s. The most common image sensors are the charge-coupled device (CCD) sensor and the CMOS (complementary MOS) active-pixel sensor (CMOS sensor). Electronic paper, which has origins in the 1970s, allows digital information to appear as paper documents. By 1976, there were several firms racing to introduce the first truly successful commercial personal computers. Three machines, the Apple II, PET 2001 and TRS-80 were all released in 1977,[54] becoming the most popular by late 1978.[55] Byte magazine later referred to Commodore, Apple, and Tandy as the ""1977 Trinity"".[56] Also in 1977, Sord Computer Corporation released the Sord M200 Smart Home Computer in Japan.[57] Steve Wozniak (known as ""Woz""), a regular visitor to Homebrew Computer Club meetings, designed the single-board Apple I computer and first demonstrated it there. With specifications in hand and an order for 100 machines at US$500 each from the Byte Shop, Woz and his friend Steve Jobs founded Apple Computer. About 200 of the machines sold before the company announced the Apple II as a complete computer. It had color graphics, a full QWERTY keyboard, and internal slots for expansion, which were mounted in a high quality streamlined plastic case. The monitor and I/O devices were sold separately. The original Apple II operating system was only the built-in BASIC interpreter contained in ROM. Apple DOS was added to support the diskette drive; the last version was ""Apple DOS 3.3"". Its higher price and lack of floating point BASIC, along with a lack of retail distribution sites, caused it to lag in sales behind the other Trinity machines until 1979, when it surpassed the PET. It was again pushed into 4th place when Atari introduced its popular Atari 8-bit systems.[58] Despite slow initial sales, the Apple II's lifetime was about eight years longer than other machines, and so accumulated the highest total sales. By 1985 2.1 million had sold and more than 4 million Apple II's were shipped by the end of its production in 1993.[59] Optical communication plays a crucial role in communication networks. Optical communication provides the transmission backbone for the telecommunications and computer networks that underlie the Internet, the foundation for the Digital Revolution and Information Age. The two core technologies are the optical fiber and light amplification (the optical amplifier). In 1953, Bram van Heel demonstrated image transmission through bundles of optical fibers with a transparent cladding. The same year, Harold Hopkins and Narinder Singh Kapany at Imperial College succeeded in making image-transmitting bundles with over 10,000 optical fibers, and subsequently achieved image transmission through a 75 cm long bundle which combined several thousand fibers. Gordon Gould invented the optical amplifier and the laser, and also established the first optical telecommunications company, Optelecom, to design communication systems. The firm was a co-founder in Ciena Corp., the venture that popularized the optical amplifier with the introduction of the first dense wave division multiplexing system.[60] This massive scale communication technology has emerged as the common basis of all telecommunication networks[61] and, thus, a foundation of the Information Age.[62][63] Manuel Castells captures the significance of the Information Age in The Information Age: Economy, Society and Culture when he writes of our global interdependence and the new relationships between economy, state and society, what he calls ""a new society-in-the-making."" He cautions that just because humans have dominated the material world, does not mean that the Information Age is the end of history: ""It is in fact, quite the opposite: history is just beginning, if by history we understand the moment when, after millennia of a prehistoric battle with Nature, first to survive, then to conquer it, our species has reached the level of knowledge and social organization that will allow us to live in a predominantly social world. It is the beginning of a new existence, and indeed the beginning of a new age, The Information Age, marked by the autonomy of culture vis-à-vis the material basis of our existence.""[64] Thomas Chatterton Williams wrote about the dangers of anti-intellectualism in the Information Age in a piece for The Atlantic. Although access to information has never been greater, most information is irrelevant or insubstantial. The Information Age's emphasis on speed over expertise contributes to ""superficial culture in which even the elite will openly disparage as pointless our main repositories for the very best that has been thought.""[65]",An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.,"As virtually every lucid American knows, Donald Trump is the presumptive Republican candidate for President. In becoming so, he has crafted his own fairy tale of a personal story, defeating sixteen major candidates along the way, many of them GOP heavyweights. When he announced his candidacy a little over a year ago, most people chuckled and presumed he would be relegated to a veritable trash bin of also-rans by the advent of the first couple of primaries. There being no shame in my game, I freely admit, I was one among the collection of most people. However, let the record reflect, I became a believer sooner than most. It is quite possible I watched the progression of the race closer than many people did. In doing so, I noticed what to me was the development of an interesting phenomenon, and it emerged quickly. In his campaign launch, Mr. Trump skewered Mexican immigrants, calling them, among other things drug dealers and rapists, as he offered, upon his personal authority, Mexico does not send us its best. He pledged to build a wall, and not just to erect it, but he insisted that he would force Mexico to pay for the wall. On its face, this seemed to cast Trump’s kickoff as not just inauspicious, but divisive, troubling, and almost certainly, quickly disqualifying. The thing is it did not play out that way. Yes, his strident commentary was a trending topic for a few days. But any great clamor of disgust and disbelief was offset by a populist attaboy that while not rising to the level of groundswell was clearly palpable. Then there was the case of the inconvenient war hero. Donald Trump questioned whether Arizona Senator John McCain, who was held as Prisoner of War (POW) for five and a half years by the North Vietnamese, was a war hero. As Trump put it, “I like people who weren’t captured.” While Senator McCain called on Trump to apologize to military families, particularly those of POW’s, he did not make a point of taking on Trump in a personal frontal assault. Several of the other sixteen candidates did though, perhaps seeing this as an opportunity to make quick work of the upstart Trump. Next up, enter South Carolina Senator Lindsey Graham, who defended his friend’s (Senator McCain) war record, and called Trump’s “antics” the last straw, and the multi-billionaire himself a jackass. Graham laid it all on the line, saying: “I can understand being frustrated about immigration, but there’s no justification to slander a bunch of people that are hardworking folks. This is a line that is crossed. I think the American people, the caucus-goers; the primary voters are going to fix this. To the other candidates, this is your chance to do the right thing and disassociate yourself from somebody who has basically shown a lack of judgment to be commander-in-chief and to be President of us all.” Shortly after Lindsey Gram’s appeal, Trump made a campaign trip to South Carolina. During the rally, in Graham’s home state, he thoroughly dissed the Senator, and topped off his rant by revealing the Senator’s cell phone number to the crowd,  a television audience, and America at large. Among the rest of the GOP field, former Texas Governor Rick Perry answered the call most fervently. He aggressively called out Trump for having attacked Senator McCain’s war hero bona fides. At worst, it was a bad move; at best, it did nothing to bolster his campaign. He could take solace in having stood up and done the right thing. Alas, by shortly after Labor Day he was forced to suspend his campaign. Even though he had significant Super PAC support, he found himself unable to raise sufficient funds through traditional campaigning to keep his run afloat. Donald J. Trump entered the 2016 Presidential race June 16, 2015. Rick Perry exited the race September 11, 2015. Once again 9/11 was a bad day, at least symbolically for Perry and his supporters. More important, in less than three months, I totally re-evaluated my position on Donald Trump and the degree to which he was a player among the Republican field. Before fall, and long before the fist primaries, I argued that Trump was not only for real, but I was certain he had a great shot to be the last person standing when the GOP dust settled. And here he stands. The question now is what, if anything, does he stand for, as he and the GOP approach this month’s Republican National Convention. I have written before about Mr. Trump’s mercurial tendencies as it relates to the issues he supports and defends…or despises and attacks. If you ask Mr. Trump, he might tell you he is famous for his brand, and that his brand is that of a wealthy, successful, did I say wealthy, very wealthy businessman. It is true that Mr. Trump owns and runs lots of businesses, and that he has earned billions of dollars. It is also true that he has lost lots of money, and filed for bankruptcy multiple times; at least four, I believe. But who’s counting? Aside from his self-proclaimed business acumen, his foray into the political arena means its fair game to take a look at his positions on the major policy issues that confront our nation on a daily basis. As commander-in-chief, he will be called upon to navigate, with the help of his cabinet, our great ship of state, through the difficult currents of war, peace, the economy, immigration, and a host of other pressing matters. With that in mind, here are ten issues upon which Mr. Trump zigged and then subsequently zagged: There are plenty more where those came from, including Teleprompters, Hillary Clinton, and building and paying for a wall, of course, just to name a few. Ten is a nice round number though; I’ll stop there. I think that’s enough to make the point, “Word Association: Trump = Quixotic (impulsive and often rashly unpredictable)!” Not exactly what you want in your next President…at least, I hope. Read my blog anytime by clicking the link: http://thesphinxofcharlotte.com. Find a new post each Wednesday. To subscribe, click on Follow in the bottom right hand corner of my Home Page at http://thesphinxofcharlotte.com; enter your e-mail address in the designated space, and click on “Sign me up.” Subsequent editions of “Break It Down” will be mailed to your in-box. http://www.ijreview.com/2016/05/607467-11-things-donald-trump-changed-his-mind-about-after-securing-gop-nomination/ http://www.latimes.com/nation/la-na-gop-trump-20150615-story.html http://www.dailykos.com/story/2015/06/17/1394019/-Trump-calls-Mexican-immigrants-drug-dealers-and-rapists-crickets-from-the-GOP-field http://www.cnn.com/2015/07/20/politics/jeb-bush-john-mccain-donald-trump/index.html http://www.cnn.com/2015/07/21/politics/donald-trump-lindsey-graham-cell-phone/index.html http://www.wsj.com/video/donald-trump-attacks-lindsey-graham-rick-perry/F6B39925-AE55-445B-8F7B-CF9B696353EE.html https://rickperry.org http://abcnews.go.com/Politics/rick-perry-donald-trump-offended-remarks/story?id=32230412 https://rickperry.org http://www.dictionary.com/browse/quixotic Nearly two decades into the 21st Century, the phrase, “It’s not about race” is still nearly always a dead giveaway that, whatever the topic du jour, it is virtually always all about race. I’ll be the first to admit that is not the way it should be, or would be in a perfect world. Alas, we are not yet there. Roughly two months ago, April 20, 2016, Treasury Secretary Jacob (Jack) Lew announced, in an open letter to the American people, that the newly redesigned $20 bill would feature abolitionist Harriet Tubman on the front. The $5 and $10 bills are also scheduled for redesign. The $10 note was due to be next in line, because of the need for security upgrades. However, as a result of an extensive 10-month survey and information gathering process, the Secretary was persuaded to leap over the less used $10 bill and move to the $20, a highly used bill that is a staple in ATM’s. Secretary Lew’s determination to put Ms. Tubman on the next $20 bill was precipitated by an overwhelming response from Americans to a poll conducted by the grassroots group, Women on 20’s. Tubman was the first choice over three other contenders, including civil rights icon Rosa Parks, former First Lady and activist Eleanor Roosevelt, and Wilma Mankiller, the first female Chief of the Cherokee Nation. About his decision, Lew said: “I have been particularly struck by the many comments and reactions from children for whom Harriet Tubman is not just a historical figure, but a role model for leadership and participation in our democracy.” Harriet Tubman was a great American shero. Her exploits as a freedom fighter are legendary. Quite naturally, she took her work most seriously. In framing the context in which she viewed the struggle, Tubman once said: “I would fight for liberty so long as my strength lasted.” Fight, she did. She fought to free slaves, and she fought for women’s suffrage. Secretary Lew gleaned through polling, cards and letters, and from his own research that Tubman’s story was one of courage and commitment to equality; one that embodied the ideals of democracy that our nation celebrates. Through placing her portrait on the obverse side of the $20 bill, we will honor her, and continue to value that legacy. The reverse side will still feature the White House and the image of President Andrew Jackson. Lew noted that the Bureau of Engraving and Printing would work closely with the Federal Reserve to speed up work on the new currency. The goal is for all three bills to go into circulation as soon as possible, consistent with security requirements. Well, that seems straight forward enough. Hold up, wait a minute! Just when it appeared that it was possible for the federal government to work smoothly and efficiently to execute the people’s business, we are slapped in the face with the all too familiar reality of an intransigent and obstructionist elected politician. Representative Steve King, Iowa, recently introduced an amendment to bar the Treasury Department from spending any funds to redesign paper money or coin currency. If this amendment were enacted, the Treasury Department’s plans to replace the image of Andrew Jackson with that of Harriet Tubman on the face of the $20 bill would be scuttled. Representative King has not publicly expressed a reason for putting forward the amendment. His office did not immediately respond to CNN’s request for comment. It is probably not news that Ms. Tubman would be the first African American to appear on American currency. While it is disappointing that Congressman King has decided, for reasons he has yet to see fit to share, to intervene and prevent the scheduled updating of our currency, despite existing security issues, I am also compelled to at least note the racial implications of his action. Am I accusing Representative King of acting in a racist manner, or of having racist motivations? No, I am not. I am merely using my powers of observation, which I surely hope are not unique, and noting an apparent (at least to me) nexus between his action, and its evident impact. National media have reported news of his amendment. As such, while you may not have heard about it, the information is implanted in the public domain. I noted in the very first sentence of this post, there is a propensity to throw a stone and hide one’s hand, when it comes to matters of race. But, as I stated then, the assertion that “it’s not about race,” is usually followed by a race-laden subject or discussion. I will not deign to characterize the representative’s motives. Instead, what I will do is point out a few of the individuals and institutions that have gone on record as opposing the placement of Ms. Tubman’s image on the $20. An abbreviated list includes: I’m sure there are others, maybe many, most, likely denizens of the GOP. For now, I will concentrate on those enumerated above. Each of them has invested considerable time, energy, and in the case of the three men, political capital, opposing any and everything even remotely associated with President Obama. Technically that does not include the Tubman initiative, since contrary to the opinion of the typical recalcitrant Obama hater; the decision-making regarding the currency was the purview of Secretary Lew, not President Obama. But, since Lew is an Obama appointee, I’ll play along and blame/credit the President for making the call. Whatever! At this point, if I were engaged in a conversation about this matter, the other party or parties would be offering a litany of thinly veiled-to-totally transparent excuses attempting to defend the non-racial basis for this amendment. On a good day I would listen politely before dismissing the excrement as the half-baked crapola that it is. On a not-so-good day…well, let’s just hope it was a good day. On this day, I’ve done my job. I have elevated the topic so that you may assess and evaluate for yourself. I leave you with this thought…”It’s Never About Race; Or Is It?” Read my blog anytime by clicking the link: http://thesphinxofcharlotte.com. Find a new post each Wednesday. To subscribe, click on Follow in the bottom right hand corner of my Home Page at http://thesphinxofcharlotte.com; enter your e-mail address in the designated space, and click on “Sign me up.” Subsequent editions of “Break It Down” will be mailed to your in-box. https://www.treasury.gov/press-center/press-releases/Pages/jl0436.aspx http://www.wmdt.com/news/politics-elections/iowa-congressman-wants-bill-to-block-20-tubman/40161176 http://www.usatoday.com/story/news/nation-now/2015/05/12/harriet-tubman-20-dollar-bill/27173135/ http://www.cnn.com/2016/06/21/politics/harriet-tubman-20-bill-steve-king/index.html http://newsone.com/3465161/iowa-republican-lawmaker-block-harriet-tubman-20-bill/ http://www.wptz.com/politics/iowa-congressman-wants-bill-to-block-20-tubman/40161176 http://www.localsyr.com/news/iowa-congressman-wants-bill-to-block-20-tubman http://www.huffingtonpost.com/entry/harriet-tubman-20-bill-steve-king_us_57687124e4b0fbbc8beb6f67 http://www.eurweb.com/2016/06/gop-congressman-seeks-block-harriet-tubman-20-bill/ http://www.omaha.com/news/iowa/steve-king-seeks-to-block-u-s-currency-redesign-including/article_13795002-37f7-11e6-8251-ef4c15429934.html http://www.blackdailynews.com/iowa-republican-lawmaker-wants-to-block-harriet-tubman-20-bill/ http://www.wmdt.com/news/politics-elections/iowa-congressman-wants-bill-to-block-20-tubman/40161176 http://www.womenon20s.org http://amendments-rules.house.gov/amendments/KINGIA_289_xml62016115509559.pdf http://downtrend.com/vsaxena/can-you-guess-why http://www.vladtv.com/article/217670/trump-opposes-tubman-on-the-20-bill-calls-it-political-correctness http://www.fox19.com/story/31777720/carson-opposes-tubman-replacing-jackson-on-20-bill http://www.wnd.com/2016/04/fox-news-star-rails-against-tubman-on-20-bill/#! Before preparing today’s post I decided to make a cursory inventory. At least six times previously, I have written about mass gun violence (in America). In the most recent instance before today I discussed the facts surrounding the June 17, 2015 shooting of the Charleston Nine at Mother Emanuel A.M.E. Church in Charleston, SC. Just two days shy of a year later, I find myself impelled to beat the drum once more. In an ironic twist, I visited the Mother Emanuel Church this weekend. As I was completing my exercise regimen, a spin bike ride, early Sunday morning, before my trip to Charleston, I read a news story and watched on CNN the story and gory details about the massacre at a gay club in Orlando named Pulse. Each time such a tragedy befalls us; we as a society are diminished. It marks yet another cruel and crushing blow to a nation that I certainly wish to see aspire to emulate its better angels rather than the very worst in our human nature. I have ranted and railed repeatedly about the role easy access to firearms plays in the frequent carnage. I’ve discussed the prevailing politics, examined the NRA and its proxies (lobbyists and Congressmen and women), and lamented the lack of reform. Been there, done that, time and time again. Not today. In honor of those 49 souls who ended their earthly assignment last Sunday morning at Pulse, I will make a brief introduction, and share the powerful words of one survivor. In the event you wish to reference my thoughts from previous posts on the subject, you can click on links below for several of them. Edward Sotomayor, Jr.: – A 34 year-old resident of Sarasota, Florida; worked at a travel agency that catered to the gay community. Stanley Almodovar, III: – A 23 year-old pharmacy technician. Hi smother made him a tomato and cheese dip that he never got to eat; he never returned home. Luis Omar Ocasio-Capo: – A 20 year-old dancer at the club. He was one of the youngest to die. Akyra Monet Murray: – An 18 year-old from Philadelphia, who was in Orlando on vacation with her brother after graduating from high school. Luis S. Vielma: – A 22 year-old who worked at Universal Orlando. J.K. Rowling, creator of the “Harry Potter” book series tweeted “I can’t stop crying.” Vielma worked on the Harry Potter ride at Universal. Juan Ramon Guerrero: – A 22 year-old worked as a telemarketer while attending the University of Central Florida. Christopher Andrew Leinonen: – A 32 year-old, and was Juan Ramon’s boyfriend. He established a gay-straight alliance at his high school. Eric Ivan Ortiz-Rivera: – A 36 year-old who had moved to Florida from Puerto Rico in pursuit of a better life. Peter O. Gonzalez-Cruz: – A 22 year-old who worked at UPS, and who was known for memorizing the names of his regular customers. Kimberley Morris: – A 37 year-old bouncer at Pulse. She enjoyed mixed martial arts and basketball. Eddie Jamoldroy Justice: – A 30 year-old accountant who lived in downtown Orlando. Enrique Rios: – A 25 year-old whom a cousin, Erick Leon, described as, “Loved by everyone who knew him.” Anthony Luis Laureano Disla: – Born in San Juan, Puerto Rico, and settled in Orlando. Jonathan Antonio Camuy Vega: – Worked in Audience Management for the current season of the singing competition at Telemundo. Yilmary Rodriguez Solivan: – A 24 year-old, was friends with Jonathan Antonio. Cory James Connell: – A 21 year-old was leaving Pulse with his girlfriend when the shooter, whose name I will not write, entered the club. Mercedes Marisol Flores: – A 26 year-old; her father Cesar is heartbroken that his daughter was killed. Deonka Deidra Drayton: – A 32 year-old, she was a bartender at Pulse. Miguel Angel Honorato: – A 30 year-old; he lived in Orlando, and managed a Mexican restaurant in Sanford, Florida. Jason Benjamin Josaphat: – A 19 year-old; he attended Southern Technical College in Orlando. Darryl Roman Burt II: – A 29 year-old; he was an employee on the Jacksonville campus of Kelser University. Juan Carlos Mendez Perez: – A 35 year-old, was the best salesperson his co-worker, Claudia Agudelo, ever met. Luis Daniel Wilson-Leon: – A 37 year-old, and a friend of Juan Carlos. Oscar A. Aracena Montero: – A 26 year-old, was celebrating the recent purchase of a home. Simon Adrian Carillo Fernandez: – A 31 year-old who was Oscar’s partner. Shane Evan Tomlinson: – A 33 year-old, who had performed with his band Frequency at another club earlier in the evening. Amanda Avear: – A 25 year-old was on Snapchat at the club when the shooting started. Martin Benitez Torres: – A 33 year-old was a student at the Tampa Bay campus of Ana G. Mendez University. Gilberto Ramon Silva Menendez: A 25 year-old from Manati, Puerto Rico who worked at a Speedway Convenience Store. He had studies Health Care Management at Ana G. Mendez University in Orlando. Javier Jorge-Reyes: – A 40 year-old from Guayama, Puerto Rico. He managed a Gucci Store in Orlando. Tevin Eurgene Crosby: – A 25 year-old, who ran his own marketing company. He graduated from West Iredell High School in Statesville, NC in 2010. Franky Jimmy Dejesus Velazquez: – A 50 year-old, he was a professional dancer. Xavier Emmanuel Serrano Rosado: – A 33 year-old, he was a dancer, and proud of his son. Joel Rayon Paniagua: – Grew up in Veracruz, Mexico; he lived near Tampa, and sent all his money home to family. Juan P. Rivera Velazquez and Luis Daniel Conde: – Velazquez 37 and a hair stylist, and Conde 39 a make-up artist, were partners. They owned a salon together. Juan Chevez-Martinez: – A 25 year-old who worked at a hotel. Jerald Arthur Wright: – A 31 year-old; he worked at Disney World. Leroy Valentin Fernandez: – A 25 year-old; he leased apartments for a living. Jean C. Nives Rodriguez: – He bought his first home just over a month ago. Rodolfo Ayalo-Ayalo: – A 33 year-old, he worked at OneBlood, a blood donation center since 2011. Brenda Lee Marquez McCool: – A 49 year-old who has 11 kids, and beat cancer twice. Her son survived the shooting. Angel L. Candelario Padro: – A 28 year-old who had recently moved to Orlando and started a job as a technician at the Florida Retina Institute. Frank Hernandez: – A 27 year-old; he worked at a Calvin Klein store. Paul Terrell Henry: – He was from Chicago, and had two children. His daughter Alexia recently graduated from high school. Antonio Davon Brown: – A 29 year-old; he was a Captain in the U.S. Army Reserves, and a graduate of Florida A&M University. Christopher Joseph Sanfeliz: – A 24 year-old, he worked at a bank. Alejandro Barrios-Martinez: – A 21 year-old, he grew up in Cuba before moving to Orlando in 2014 to live with his father. Geraldo A. Ortiz-Jimenez: – A 25 year-old, known as “Drake” to his friends, was a native of Santo Domingo, Dominican Republic. That completes a listing of biographical summaries of the 49 men and women who lost their lives a Pulse, as shared by CNN. It is without question, their abrupt departure from life, and especially from their friends and loved ones, is a tragedy of incalculable magnitude. I will not even attempt to put that into words. Instead, I will rely upon the insight and heart-rending sentiments of one survivor, Ms. Patience Carter. “Wanting to smile about surviving but not sure if the people around you are ready. As the world mourns, the victims killed and viciously slain, I feel guilty about screaming about my legs in pain. Because I could feel nothing like the other 49 who weren’t so lucky to feel this pain of mine. I never thought in a million years that this could happen.I never thought in a million years that my eyes could witness something so tragic. Looking at the souls leaving the bodies of individuals. Looking at the killer’s machine gun throughout my right peripheral. Looking at the blood and debris covered on everyone’s faces. Looking at the gunman’s feet under the stall as he paces. The guilt of feeling lucky to be alive is heavy. It’s like the weight of the ocean’s walls crushing uncontrolled by levies. It’s like being drug through the grass with a shattered leg and thrown on the back of a Chevy. It’s like being rushed to the hospital and told you’re gonna make it when you laid beside individuals whose lives were brutally taken. The guilt of being alive is heavy.” With that powerful personal testimony, I think it’s all been said. “This Time Orlando: “Pulse” of the Nation!” Read my blog anytime by clicking the link: http://thesphinxofcharlotte.com. Find a new post each Wednesday. To subscribe, click on Follow in the bottom right hand corner of my Home Page at http://thesphinxofcharlotte.com; enter your e-mail address in the designated space, and click on “Sign me up.” Subsequent editions of “Break It Down” will be mailed to your in-box. https://en.wikipedia.org/wiki/2016_Orlando_nightclub_shooting http://www.tampabay.com/news/publicsafety/crime/in-his-own-words-pulse-nightclub-shooting-survivor-patience-carter/2281662 https://en.wikipedia.org/wiki/Pulse_(nightclub) http://www.cnn.com/2016/06/13/us/orlando-victims-profiles/index.html http://www.nytimes.com/2016/06/14/us/reconstruct-orlando-pulse-shootings.html?_r=0 https://en.wikipedia.org/wiki/Charleston_church_shooting https://thesphinxofcharlotte.com/2015/06/24/carnage-in-a-charleston-church-another-mass-murder/ https://thesphinxofcharlotte.com/2014/06/18/dead-wrong/ https://thesphinxofcharlotte.com/2014/05/28/another-shooting-pervades-the-american-psyche/ https://thesphinxofcharlotte.com/2012/12/19/this-time-will-be-different/ http://thesphinxofcharlotte.blogspot.com/2012/07/james-holmes-darkest-knight.html https://www.blogger.com/blogger.g?blogID=5516202214514810687#editor/target=post;postID=474307075967394496;onPublishedMenu=posts;onClosedMenu=posts;postNum=256;src=postname Before moving to concentrate on today’s post, I want to highlight an issue related to the special significance to the current political narrative. On Monday evening, CNN and the Associated Press declared that Hillary Clinton has crossed the threshold to reach the combined number of Pledged and Super Delegates requited to clinch the Democratic Presidential Nomination. The New York Times added its imprimatur yesterday morning. That was all before taking into account the six Primary contests conducted yesterday in California, Montana, New Jersey, New Mexico, North Dakota, and South Dakota. The candidates are expected to split the 694 available delegates. Next Tuesday, Washington, DC will officially close out the 2016 Primary Election season. Twenty delegates are at stake there. Let me be clear, the quest for determining the Democratic Party’s nominee for President ends “officially” in Philadelphia on July 28th, at the Democratic National Convention. That is an incontrovertible fact. There are expected to be 4,765 delegates to the Democratic National Convention, so a candidate needs a simple majority of 2,383 delegates to win the presidential nomination. However…for all practical purposes, the contest ended Monday evening, when Hillary Rodham Clinton became the Party’s presumptive nominee. Bernie Sanders has done an incredible job with his campaign. He exceeded expectations, most likely, including his own. He engaged youth and Millennials in historic fashion. He has vowed to fight on until the Convention. At the outset, Senator Sanders argued that the country was poorly served by crony Capitalism and the Party’s reliance on big money donors, and its penchant for rewarding the financial industry with bank bailouts, the auto industry with its own bailout, and generally dismissing regular middle class Americans. His message resonated with many Americans and as a result, he built a small donation based Campaign that rivaled the big PACs, in fundraising, and enabled the Senator to take his Campaign through the entire Primary Season in a competitive way. Alas, there was one major problem. Secretary Clinton held her own in the early contests in the Northeast, built a sizable lead in the South, and held on and in some cases expanded her margin as contests moved to the Midwest and the West. As she maintained and/or expanded her lead, the Senator was forced to pivot. He could no longer focused solely on the crony Capitalism argument; he added that the Party electoral process was also corrupt. Since then, the Sanders Campaign has made a huuuuge issue of the Democrat’s Super Delegate process. He and his supporters also skewered the Democratic National Committee (DNC) Chairman, Debbie Wasserman Schultz, who is a Clinton supporter. Undoubtedly, the next seven weeks will be filled with intrigue and maneuvering as the Clinton and Sanders campaigns navigate and negotiate a truce that in the end must produce something between Détente and a unified Party. The 2016 General Election is scheduled for Tuesday, November 8th, five months from yesterday. You can be sure I will have a lot to say about politics and the election in the coming weeks and months. In the meantime, with all due deference to Bernie Sanders for conceiving and constructing an outstanding and an incredibly productive effort, Secretary Clinton has fashioned a lead among Pledged Delegates, among Super Delegates, in the popular vote, and in the number of state contests won. Congratulations to Hillary Rodham Clinton on positioning herself to become the first female Presidential Nominee of a major Party in American History. That highlight was longer than I intended. Let me move directly to addressing the main topic. As a nation, we have become a prickly lot. I’ve written volumes about the notion some Americans label us an Exceptional nation. Indeed, we have amassed many accomplishments that render us distinguished. Yet, I must inject, America has long held a rather high-minded view…of itself. A home team media; a biased political class of powerful elites armed with a self-serving agenda, often crafts this narrative. Regardless of its origin, millions of Americans routinely buy into or co-sign such views. Regardless of the appellation applied to a particular age, whether in this country or others, the name often is intended to convey some positive aspect or attribute of society, or of it’s people. That is as true for the largely European Age of Enlightenment as it is for the current worldwide Information Age. As you may know, the Information Age, also known as the Computer Age, Digital Age, or New Media Age) is a period in human history characterized by the shift from traditional industry that the Industrial Revolution brought through industrialization, to an economy based on information computerization. The onset of the Information Age is associated with the Digital Revolution, just as the Industrial Revolution marked the onset of the Industrial Age. I want to momentarily elevate a different kind of Age, the Age of Incivility. In a paradoxical kind of way, President Barack Obama’s critics frequently credit or blame, you decide which is appropriate, him for the rise of incivility in America. While I am inclined to characterize any such attributions as bovine excrement, I do see how, and in select instances why, some folks might feel that way. Donald Trump has been the Presumptive Republican Nominee for President for several weeks now. Mr. Trump bogarted his way to the top of the heap of GOP Presidential candidates by relentlessly attacking his fellow competitors, as well as a host of others, including Mexicans and Muslims. Mr. Trump has vacillated between controversial and flat out toxic, even in his own Party. How controversial/toxic has he been? Just yesterday, Republican Paul Ryan, Speaker of the U.S. House of Representatives, called Trump’s comments on (Kappa Alpha Psi Man) Judge Gonzalo Curiel “indefensible,” “wrong” and “racist.” At a news conference in the Anacostia section of Washington, DC yesterday, with a full complement of African Americans in attendance, Representative Ryan said, “Claiming that a person can’t do their job because of their race is sort of like the textbook definition of a racist comment.” Trump suggested Curiel could not adjudicate his case without bias because “He is Mexican, and I am building a wall.”   For the record, Judge Curiel was born in Indiana. “No, I’m not – I’m saying that the comment was. I don’t know what’s in his heart, I can’t speak to that whatsoever. What I’m saying is to suggest that a person’s race disqualifies them to do their job is textbook – that’s what I’m saying. I’m not saying what’s in his heart because I don’t know what is in his heart and I don’t think he feels that in his heart but I don’t think it is wise or justifiable to suggest that a person should be disqualified from their job because of their ethnicity.” Despite his frustration, Ryan, who initially declined to endorse Trump said he would still vote for him. This exemplifies the challenge leaders of the GOP face. They appreciate the demographic filters associated with winning the White House. They also know, unquestionably, what it is like to lose two White House bids in a row. I am certainly not going to say Donald Trump cannot be the next President. He does, however, continue to take actions and make statements that exasperate those in his own Party, who possibly want that high office more for him than it often looks as though he wants it for himself. In retrospect, watching and listening to Ryan alternately prod Candidate Trump to be better, tiptoe around inconvenient truths, and dissemble with passion, all in an apparent effort to maintain a shred of credibility, I am reminded of the Biblical admonition found in Matthew 19:14, which advises, “Again I tell you, it is easier for a camel to go through the eye of a needle than for someone who is rich to enter the Kingdom of God.” (–KJV) I think of this passage, not because of Mr. Trump’s wealth, though, as he frequently reminds us, he has done very, very well in amassing a fortune, but rather due to his propensity to spew venomous uncivil statements. To close the metaphor, it appears from my vantage point, it is more difficult for a camel to traverse the eye of a needle than for Paul Ryan and the GOP Establishment to corral and manage TrumpSpeak. Republicans leadership has spent 7 and a half years demonizing and lambasting President Obama. The level of disrespect has been so prevalent. How pervasive and disrespectful has it been? From SC Congressman Joe Wilson’s 2011 “You lie” comment, directed at the President as he addressed a joint session of Congress on various aspects of the Affordable Care Act, to then House Speaker John Boehner’s, Ohio, bypassing the President and issuing an invitation to Benjamin “Bibi” Netanyahu to address Congress in 2015, to Arkansas Senator Tom Cotton initiating a letter to the leaders of Iran, signed by every Republican Senator except, Lamar Alexander and Bob Corker of Tennessee, Dan Coats of Indiana, Thad Cochran of Mississippi, Susan Collins, of Maine, and Jeff Flake and Lisa Murkowski of Alaska, also last year. It is because of an innumerable list of reasons like those above that I have noted many times that Trump is effectively the anointed one; the anti-Obama, if you will. Therefore, it becomes a natural progression that the newly minted presumptive Republican Nominee takes off where his recently adopted Party left off. So yes, Obama haters can blame the President as often and as fervently as they like. Just be mindful, calling a pigeon a pimento cheese sandwich doesn’t make it be one. Historically, we like to summon data rather than rely on “a feeling” to underscore the most vital of points. To that end, I submit two studies that suggest support for Trump is highly correlated to concerns about race and ethnicity. In one study, Hamilton College political scientist Philip Klinkner analyzed data the 2016 American National Election Study in a representative sample of 1,200 Americans to compare feelings toward Donald Trump and Hillary Clinton. He evaluated the degree to which economic opinions, racial attitudes, and demographic variables predicted an individual’s feelings toward the two. His research showed one factor was much stronger than the others: “My analysis indicates that economic status and attitudes do little to explain support for Donald Trump. Those who express more resentment toward African Americans, those who think the word ‘violent’ describes Muslims well, and those who believe President Obama is a Muslim have much more positive views of trump compared with Clinton.” Moving from the least to the most resentful view of African Americans increases support for Trump by 44 points, those who think Obama is a Muslim (54% of all Republicans) are 24 points more favorable to Trump, and those who think the word “violent” describes Muslims extremely well are about 13 points more pro-Trump than those who think it doesn’t describe them well at all. In the second study, the Washington Post conducted a similar analysis using data from a national poll co-sponsored by ABC News comparing Trump’s support to the other Republican primary candidates. The survey questions asked Republicans and Republican-leaning voters whether they themselves were struggling economically, and whether white people’s troubles were a direct result of “preferences for blacks and Hispanics.” The biggest predictor of Trump support among Republicans and Republican-leaning voters was a belief that “the growing number of newcomers from other countries threatens U.S. values.” Republicans holding this belief felt 18 points more positive toward Trump on a 100-point scale, than Republicans who didn’t feel this way. Belief that Islam encourages violence, and that it’s “bad” for the country that blacks, Latinos and Asians will someday make up the majority of the population, accounted for eight-point jumps in positive feelings toward Trump. In summary, it’s about to go down. It’s up to you to fight, and especially vote to change this dynamic if you believe it’s inappropriate. For the time being, what we all face is…”No Middle Ground: Welcome to the Age of Incivility!” Read my blog anytime by clicking the link: http://thesphinxofcharlotte.com. Find a new post each Wednesday. To subscribe, click on Follow in the bottom right hand corner of my Home Page at http://thesphinxofcharlotte.com; enter your e-mail address in the designated space, and click on “Sign me up.” Subsequent editions of “Break It Down” will be mailed to your in-box. https://www.washingtonpost.com/news/wonk/wp/2016/06/06/racial-anxiety-is-a-huge-driver-of-support-for-donald-trump-two-new-studies-find/ http://www.cnn.com/2016/06/07/politics/hillary-clinton-democratic-nomination/index.html http://www.cnn.com/2016/06/07/opinions/the-race-hillary-clinton-has-already-won-ellen-fitzpatrick/index.html https://en.wikipedia.org/wiki/2016_Democratic_National_Convention https://en.wikipedia.org/wiki/Debbie_Wasserman_Schultz http://www.thedailybeast.com/articles/2013/11/12/how-jackie-kennedy-invented-the-camelot-legend-after-jfk-s-death.html https://en.wikipedia.org/wiki/Age_of_Enlightenment https://en.wikipedia.org/wiki/Postmodernism https://en.wikipedia.org/wiki/Reconstruction_Era https://en.wikipedia.org/wiki/Gilded_Age https://en.wikipedia.org/wiki/Progressive_Era https://en.wikipedia.org/wiki/Information_Age https://en.wikipedia.org/wiki/List_of_time_periods#The_Americas http://www.caller.com/opinion/letters-to-editor/letter-obama-launched-age-of-incivility-2961d284-4060-3ac4-e053-0100007fea04-365469371.html https://en.wikipedia.org/wiki/Paul_Ryan https://mic.com/articles/145522/paul-ryan-blasts-donald-trump-s-racist-attacks-on-judge-but-stands-by-endorsement#.xD8mNInRJ https://en.wikipedia.org/wiki/Democratic_National_Committee (Please enjoy this reprised/updated edition of “Break It Down!” This post was originally published May 30, 2012 at: http://thesphinxofcharlotte.blogspot.com) OK, so Memorial Day was earlier this week. You may be familiar with my holiday week philosophy, which is: make it easy on the readers, who are always otherwise engaged, no matter the holiday. Of course, in the process, I am also giving myself a break. That makes for a natural win-win scenario. Memorial Day is a federal holiday, observed the last Monday in May, to honor America’s fallen soldiers. It originated after the Civil War. Falling between Easter and Independence Day, it is often equated with a late spring break, or a pre-summer respite. In fact, many consider it the unofficial first weekend of summer, contrasted with the Labor Day Holiday Weekend, which for many signals the unofficial end of summer. The holiday weekend typically includes a cornucopia of sports. For example Memorial Day 2016 weekend’s events included matches from the  French Open, the NASCAR Coca-Cola 600, a Game 7 in the NBA Conference Finals, College Men’s Baseball playoffs, College Women’s Softball competition, and Championships in Men’s and Women’s Collegiate Lacrosse, coincidentally, both won by teams from the University of North Carolina, among other sports. With the rapidly heating-up political season thrown in the mix, the holiday is sometimes almost lost in the shuffle, especially this year with Donald Trump totally reinventing the GOP Presidential race, and Bernie Sanders fashioning his own brand of revolution in the Democratic Party’s race. But wait; Memorial Day also has a special cultural significance. In fact, it is because of that nexus we should pay special homage to this late spring holiday. The first well-known observance of a Memorial Day type was held May 1, 1865 in Charleston, South Carolina. Over 250 Union soldiers that had been prisoners of war, died in Charleston, and were quickly buried in makeshift graves. A group of blacks, mostly freedmen, organized the observance and led cleanup and landscaping of the burial site. Most of the nearly 10,000 people who attended were freedmen and their families. Of that number, 3.000 were children, newly enrolled in freedman’s schools. Mutual aid societies, black ministers, and white Northern missionaries were also in attendance. David W. Blight, Professor of American History at Yale University, and Director of the school’s Gilder-Lehrman Center for the Study of Slavery, Resistance, & Abolition, described the day this way: “This was the first Memorial Day. African Americans invented Memorial Day in Charleston, South Carolina. What you have there is black Americans recently freed from slavery announcing to the world with their flowers, their feet, and their songs what the War had been about. What they basically were creating was the Independence Day of a Second American Revolution.” Professor Blight conceded there is no evidence that the Charleston event led directly to the establishment of Memorial Day across the country. But the record is clear they formed the earliest truly large-scale event, complete with media coverage. Their effort was the prototype, if not the catalyst. Read my blog anytime by clicking the links: http://thesphinxofcharlotte.com or http://thesphinxofcharlotte.blogspot.com. A new post is published each Wednesday. For more detailed information on a variety of aspects relating to this post, consult the links below: http://en.wikipedia.org/wiki/Memorial_Day http://en.wikipedia.org/wiki/David_W._Blight http://www.davidwblight.com/ http://www.snopes.com/military/memorialday.asp http://www.dailykos.com/story/2014/05/25/1301862/-Memorial-Day-Has-African-American-Roots-First-One-Was-Conducted-By-Former-Slaves# http://en.wiktionary.org/wiki/KISS_principle http://en.wikipedia.org/wiki/KISS_principle http://en.wikipedia.org/wiki/American_Civil_War http://www.yale.edu/glc/index.htm http://www.civilwarhome.com/freedmen.htm",An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.,,,,,
